{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "suppressPackageStartupMessages(library(tidyverse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check your bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A risky insurance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This exercise tackles the important task of detecting possible sources of discrimination in statistical modelling. The dataset we'll work on regards an insurance provider: the response variable that we are trying to determine is the cost of a certain insurance (how much an individual should pay for the insurance); the predictive variables cover a set of health (how much do you smoke? do you brush your teeth? do you wear glasses? did you have heart problems?), occupational (are you employed? is your job stressful? is your job dangerous? do you work by night? how much do you earn?) and demographic (age, gender, ethnicity, ZIP code) informations.\n",
    "\n",
    "**_pedagogical goals_**: The series of exercise should encourage the student to consider that staticals, and machine learning, models can treat unfairly their data subjects. Detecting it is not trivial, but something can and should be attempted. A set of tecniques is also illustrated. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ex 2.1 A fair split\n",
    "\n",
    "The learner is given a Decision Tree model, and asked to detect possible discrimination issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_solution_ the Decision Tree directly uses personal data variable to discriminate by ethnicity and gender. The learner can detect this by printing the Decision Tree and looking at the variables used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ex 2.2 The bias in the forest\n",
    "\n",
    "The learner is given a Random Forest model, and asked to detect possible discrimination issues. We already give the code to identify which variables are used to determine the cost of the insurance. The student is asked plot the data to identify if any of the variable used in the Random Forest may produce a discriminating result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_solution_ The dataset comes from a segregated area: ZIP codes can, with high accuracy, identify the ethnicity of the data subjects. Insurance costs are higher for the ZIP codes associated with certain ethnicity. This is a discrimination by proxy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ex 2.3 A black box can be biased\n",
    "\n",
    "The learner is given a rather obscure Neural Network model. None of the input variable in the model are clear discriminating (directly or by proxy). This is a coding exercise in which the learner is asked to [complete? reorder? write?] some code to directly check whether the model ends up being fair or not. This is done by feeding different population of observation in the model and considering the distribution of insurance costs. If I manage to build the dataset properly, it should elicit the importance of thinking about intersectionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
