Ethics concept: bias

The following was collected from X census survey.
<insert dataframe with gender, age, location, other variables>

Examine the data. Q: Which variable could potential contain bias?
[x] gender
[] Age
[] city

Explanation: The sex question only gave the option of M(male) and F(female)


The same survey was give to a similar subset of the population. This time the options for gender were M, F, Neither.
<insert dataframe>

Examine/explore the data. How does it compare to the first dataset?



The same survey was give to a similar subset of the population. This time the options for gender were M, F, Other.
<insert dataframe>

Examine/explore the data. How does it compare to the first dataset?

Is there a signifigant difference in the gender demographics between the Neither and the Other?
What variable is the difference linked to?
[] city
[] race
[] language

Notes: I'm thinking of how the non-binary asignation is not standard yet and it would be interesting if the difference between Other and Neither is significant in some way, maybe intersected with race. We could look it up but maybe Latinos put neither and Whites put other.



Return to the first data set (M, F).
<insert dataframe>

Is there a way to mitigate the bias?
[]Find another dataset to intersect with the survey data that shows a more detailed picture of gender.
[] Apply a blanket statistic that assigns the blank answers to non-binary answers. 

Write code that intersects the data set 1 with the new data set.

Write code that intersects the data set 2 with the new data set.

Write code that intersects the data set 3 with the new data set.


Which approach might you chose based on what you have learned above?
